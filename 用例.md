该小结是对Apache Kafka®的一些流行用例的描述。有关这些领域的概述，请参阅此博客文章。

### 消息

Kafka可以很好地替代传统message broker。Message broker的使用有多种原因（将处理与数据生产者分离，缓冲未处理的消息等）。与大多数消息系统相比，Kafka具有更好的吞吐量、内置的分区、复制和容错能力，这使其成为大规模消息处理应用程序的理想解决方案。
根据我们的经验，messaging的使用通常吞吐量较低，但是实际可能需要较低的端到端延迟，这通常依赖于Kafka提供的强大持久性保障。
在此领域中，Kafka与ActiveMQ或 RabbitMQ等传统消息传递系统相当。

### 网站活动跟踪

Kafka最初的用例是能够将用户活动跟踪管道重建为一组实时的发布-订阅feeds。这意味着网站活动（页面浏览、搜索或用户可能采取的其他操作）被发布到中心topic，每种活动类型只有一个topic。这些feeds可由一系列用例的订阅，包括实时处理、实时监控，以及加载到Hadoop或离线数据仓库系统中以进行离线处理和报告。
活动跟踪量通常很大，因为每个用户页面视图都会生成许多活动消息。

### Metrics

Kafka通常用于操作监控数据。这涉及到汇总来自分布式应用程序的统计信息，以生成操作数据的集中feeds。

### 日志汇总

许多人使用Kafka代替日志聚合解决方案。日志聚合通常从服务器收集物理日志文件，并将它们放在中央位置（也许是文件服务器或HDFS）以进行处理。Kafka提取文件的详细信息，并以日志流的形式更清晰地抽象日志或事件数据。这允许较低延迟的处理，并更容易支持多个数据源和分布式数据消费。与Scribe或Flume等以日志为中心的系统相比，Kafka具有同样出色的性能，由于复制而提供的更强的健壮性保证以及更低的端到端延迟。

### 流处理

Kafka的许多用户在由多个阶段组成的pipeline中处理数据，其中原始输入数据从Kafka topic中消费，然后进行汇总、充实或以其他方式转换至新的topic，以供进一步消费或后续处理。例如，推荐新闻文章的pipeline可能会从RSS feeds中检索文章内容，然后将其发布到"文章"topic中。进一步的处理可能会对该内容进行规范化或重复数据删除，并将清洗后的文章内容发布到新的topic中；最后的处理阶段可能会尝试将此内容推荐给用户。这样的pipeline基于各个topic创建实时数据流图。从0.10.0.0开始，一个轻量但功能强大的流处理库Kafka Streams 可以在Apache Kafka中用来执行上述数据处理。除了Kafka Streams以外，其他开源流处理工具还包括Apache Storm和 Apache Samza。

### 事件驱动

事件驱动是一种应用程序设计风格，状态更改以时间顺序记录。Kafka大量存储的日志数据使其成为以这种风格构建应用程序的绝佳后端。

### 提交日志

Kafka可以用作一种分布式系统的外部提交日志。该日志有助于在节点之间复制数据，并充当故障节点恢复其数据的重新同步机制。Kafka中的日志压缩功能有助于支持此用法。在这种用法中，Kafka与Apache BookKeeper项目类似。

